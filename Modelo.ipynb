{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhlLEhpdSiyE",
        "outputId": "622b8809-2d74-4215-f3f5-9f39765c1de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip install unidecode\n",
        "!pip install pydot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UoGZe_ivzWP",
        "outputId": "96961536-a6d8-46a3-83e6-ee24cb808a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import datetime\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "import operator\n",
        "\n",
        "from google.colab import drive\n",
        "from unidecode import unidecode\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import Birch\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set()\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fqfCavmysyA"
      },
      "source": [
        "## Manipulação de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR1dIn5a7l7w"
      },
      "source": [
        "###1 - Criação de informação sobre dias totais do processo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkRgdo05ytVW"
      },
      "source": [
        "A função ```datas_df```:\n",
        "\n",
        "*   Cria uma coluna chamada \"data_final\", com base na coluna \"data_ultimo_mov\"\n",
        "*   Cria uma coluna chamada \"tempo_proc_dias\", a partir das duas colunas anteriores\n",
        "* Usa outras três funções auxiliares \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ovouP-VT2O"
      },
      "source": [
        "# Faz a preparação do dataframe, removendo valores e criando colunas\n",
        "def prep_dias_proc(df):\n",
        "    # Cria novas colunas com 0\n",
        "    df['tempo_proc_dias'] = 0\n",
        "    df['data_ultimo_mov'] = 0\n",
        "\n",
        "    # Retira linhas onde os movimentos são 0 (não há informação sobre eles)\n",
        "    df = df[df['movimentoLocal'] != 0]\n",
        "    df = df[df['movimentoNacional'] != 0]\n",
        "\n",
        "    # Reseta o índice\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Cria uma coluna chamada \"data_ultimo_mov\", contendo o movimento mais antigo do dataframe\n",
        "def coluna_ultimo_mov(df_m):\n",
        "    # Inicializando a variável de índice e a lista que armazenará os movimentos\n",
        "    index = 0\n",
        "    lst_movs_date = []\n",
        "\n",
        "    # Percorre todas as linhas do df\n",
        "    for linha in df_m['movimentoLocal']:\n",
        "        # Verifica se a coluna é uma lista vazia\n",
        "        if df_m.loc[index, 'movimentoLocal'] != []:\n",
        "            # Percorre as listas dentro da coluna\n",
        "            for lista in df_m.loc[index, 'movimentoLocal']:\n",
        "                # Pega o terceiro valor (data-hora no formato ISO)\n",
        "                date_mov = lista[2]\n",
        "                lst_movs_date.append(date_mov)\n",
        "\n",
        "        # Mesmos passos, só que para a coluna de movimentos nacionais\n",
        "        if df_m.loc[index, 'movimentoNacional'] != []:\n",
        "            for lista in df_m.loc[index, 'movimentoNacional']:\n",
        "                date_mov = lista[1]\n",
        "                lst_movs_date.append(date_mov)\n",
        "        \n",
        "        # Faz sorting da lista de strings. Como está no formato ISO, a ordenação pode ser feita como string\n",
        "        lst_movs_date = sorted(lst_movs_date)\n",
        "\n",
        "        # Pega o último elemento da string (o maior valor) e o imputa na coluna de último movimento\n",
        "        oldest = lst_movs_date[-1]\n",
        "        df_m.loc[index, 'data_ultimo_mov'] = str(oldest)\n",
        "        \n",
        "        # Esvazia a lista e incrementa o índice\n",
        "        lst_movs_date = []\n",
        "        index+=1\n",
        "\n",
        "    return df_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbREBw5c27u"
      },
      "source": [
        "# Função principal. Cria o df com a coluna de dias do processo e o retorna.\n",
        "def datas_df(df_d):\n",
        "    \n",
        "    # Chama as funções auxiliares\n",
        "    df_d = prep_dias_proc(df_d) \n",
        "    df_d = coluna_ultimo_mov(df_d)\n",
        "\n",
        "    index = 0\n",
        "    \n",
        "    # Percorre as linhas do df, captando o valor da coluna 'dataAjuizamento'\n",
        "    for datahora in df_d['dataAjuizamento']:\n",
        "        # Primeiro movimento: data de ajuizamento\n",
        "        inic = datahora\n",
        "        # Último movimento é o de data mais recente\n",
        "        fim = df_d.loc[index, 'data_ultimo_mov']\n",
        "        \n",
        "        \n",
        "        # Convertendo em datetime para poder realizar operação\n",
        "        # Se estiver no formato ISO\n",
        "        if len(inic) == 14:\n",
        "          d_inicial = datetime.datetime.strptime(inic, '%Y%m%d%H%M%S')\n",
        "        # Se estiver no formato UNIX Epoch (millisecs desde 1/1/1970)\n",
        "        elif len(inic) == 12:\n",
        "          d_inicial = datetime.datetime.strptime(datetime.datetime.utcfromtimestamp(int(int(inic)/1000)).isoformat(), '%Y-%m-%dT%H:%M:%S')\n",
        "        # Se não for nenhum dos anteriores, eliminar a linha pois ela não contém informação\n",
        "        # de datas facilmente acessível\n",
        "        # Obs.: Não resetar o Index devido ao loop, se o Index fosse resetado seriam puladas \n",
        "        # tantas linhas quantas fossem removidas, pois a variável 'index' é incrementada\n",
        "        # em todos os 3 casos\n",
        "        else:\n",
        "          lst = df_d[df_d.dataAjuizamento==inic].index.tolist()\n",
        "          for i in lst:\n",
        "            df_d.drop(i)\n",
        "          continue\n",
        "        d_final = datetime.datetime.strptime(fim, '%Y%m%d%H%M%S')\n",
        "        \n",
        "        # Preenchendo a coluna com os dias entre a data final e a data inicial\n",
        "        df_d.loc[index, 'tempo_proc_dias'] = (d_final - d_inicial).days\n",
        "\n",
        "        index+=1\n",
        "    return df_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKu5yeMOAOx2"
      },
      "source": [
        "### 2 - Total de movimentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Zq0QQ2ARtB"
      },
      "source": [
        "# Cria as colunas \"total_movs_loc\", \"total_movs_nac\" e \"total_movs\"\n",
        "def total_movimentos(df):\n",
        "    df['total_movs_loc'] = np.nan\n",
        "    df['total_movs_nac'] = np.nan\n",
        "    df['total_movs'] = np.nan\n",
        "    index = 0\n",
        "    \n",
        "    for i in df['movimentoNacional']:\n",
        "\n",
        "        try:\n",
        "            df.loc[index, 'total_movs_nac'] = len(df['movimentoNacional'][index])\n",
        "        except:\n",
        "            df.loc[index, 'total_movs_nac'] = 1\n",
        "        try:\n",
        "            df.loc[index, 'total_movs_loc'] = len(df['movimentoLocal'][index])\n",
        "        except:\n",
        "            df.loc[index, 'total_movs_loc'] = 1\n",
        "        try:\n",
        "            df.loc[index, 'total_movs'] = len(df['movimentoLocal'][index]) + len(df['movimentoNacional'][index])\n",
        "        except:\n",
        "            df.loc[index, 'total_movs'] = 1\n",
        "        \n",
        "        index+=1\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw0rCESnT1Uq"
      },
      "source": [
        "###3 - Alteração coluna grau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzEcxYDFT4QE"
      },
      "source": [
        "def altera_grau(df):\n",
        "    filtro_G1 = df['grau'] == 'G1'\n",
        "    filtro_G2 = df['grau'] == 'G2'\n",
        "    # Se grau != G1 && grau != G2 == Se !(grau == G1 || grau == G2), pois o grau \"G3\" está \n",
        "    # registrado como \"SUP\" ou \"Sup\"\n",
        "    filtro_G3 = np.logical_not(np.logical_or(filtro_G1, filtro_G2))\n",
        "\n",
        "    df.loc[filtro_G1, 'grau'] = 1\n",
        "    df.loc[filtro_G2, 'grau'] = 2\n",
        "    df.loc[filtro_G3, 'grau'] = 3\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjSxA2Hcgi4g"
      },
      "source": [
        "###4 - Colunas movimentos mais comuns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWv4A099gl63"
      },
      "source": [
        "# Contador de freq de movs nacionais\n",
        "def cols_movs_comuns(df4, com_desc=True):\n",
        "    df4[['1_mov_nac_freq', '2_mov_nac_freq']] = 0\n",
        "    dict_movs = {}\n",
        "    index = 0\n",
        "\n",
        "    for linha in df4['movimentoNacional']:\n",
        "        if df4.loc[index, 'movimentoNacional'] != []:\n",
        "            for lista in df4.loc[index, 'movimentoNacional']:\n",
        "                movimento = lista[0]\n",
        "\n",
        "                if movimento in dict_movs:\n",
        "                    # Incremento na contagem\n",
        "                    dict_movs[movimento] += 1\n",
        "                else:\n",
        "                    # Recebe o valor 1 (primeira ocorrência)\n",
        "                    dict_movs[movimento] = 1\n",
        "\n",
        "            # Pegando o maior valor\n",
        "            maior = max(dict_movs.items(), key=operator.itemgetter(1))[0]\n",
        "            df4.loc[index, '1_mov_nac_freq'] = maior\n",
        "\n",
        "            # Excluindo e fazendo nova verificação de maior valor\n",
        "            dict_movs.pop(maior, None)\n",
        "            try:\n",
        "                maior = max(dict_movs.items(), key=operator.itemgetter(1))[0]\n",
        "                df4.loc[index, '2_mov_nac_freq'] = maior\n",
        "            except:\n",
        "                # Erro ocorre em processos com apenas um movimento nacional\n",
        "                df4.loc[index, '2_mov_nac_freq'] = 0\n",
        "        index+=1\n",
        "        dict_movs = {}\n",
        "    \n",
        "    # Cria duas janelas com descrições dos movimentos mais comuns no processo.\n",
        "    if com_desc:\n",
        "        df4[['desc_mov1', 'desc_mov2']] = '0'\n",
        "\n",
        "        # Abre o .csv de movimentos para fins de tradução dos códios\n",
        "        path = '/content/drive/My Drive/Dados/movimentos.csv'\n",
        "        df_movimentos = pd.read_csv(path)\n",
        "\n",
        "        index2 = 0\n",
        "        for item in df4['1_mov_nac_freq']:\n",
        "            mov1 = int(df4.loc[index2, '1_mov_nac_freq'])\n",
        "            mov2 = int(df4.loc[index2, '2_mov_nac_freq'])\n",
        "\n",
        "            if mov1 != 0:\n",
        "                try:\n",
        "                    df4.loc[index2, 'desc_mov1'] = df_movimentos.loc[df_movimentos['codigo']==mov1, 'descricao'].values.item()\n",
        "                except:\n",
        "                    df4.loc[index2, 'desc_mov1'] = 0\n",
        "            if mov2 != 0:\n",
        "                try:\n",
        "                    df4.loc[index2, 'desc_mov2'] = df_movimentos.loc[df_movimentos['codigo']==mov1, 'descricao'].values.item()\n",
        "                except:\n",
        "                    df4.loc[index2, 'desc_mov2'] = 0\n",
        "\n",
        "            index2+=1\n",
        "\n",
        "    return df4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYcEUoMl8dP6"
      },
      "source": [
        "# Função Wrapper para aplicar as 4 acima em ordem\n",
        "def deploy(trtnum: str):\n",
        "  with open('drive/My Drive/Dados/Processos/processos-trt'+trtnum+'.pkl','rb') as f:\n",
        "    df = pd.read_pickle(f)\n",
        "\n",
        "  print(\"Datas\")\n",
        "  df = datas_df(df)\n",
        "  print(\"MovimentosTotal\")\n",
        "  df = total_movimentos(df)\n",
        "  print(\"Grau\")\n",
        "  df = altera_grau(df)\n",
        "  print(\"MovimentosColunas\")\n",
        "  df = cols_movs_comuns(df)\n",
        "  print(\"tempoMedioMovimentos\")\n",
        "  df['tempoMedioEntreMovimentos'] = df.apply(lambda row : row['tempo_proc_dias']/row['total_movs'], axis = 1)\n",
        "\n",
        "\n",
        "  with open('drive/My Drive/Dados/Processos/processos-trt'+trtnum+'.pkl','wb') as f:\n",
        "    df.to_pickle(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99zSUG1weaA"
      },
      "source": [
        "Chamando as funções declaradas acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGFJvgKY8MyN"
      },
      "source": [
        "for i in range(1,26):\n",
        "  deploy(str(i))\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1DUsQ2nqPp",
        "outputId": "681cc741-de47-4661-d88c-a3e5f76ba7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Checando a shape para verificar se as colunas foram criadas corretamente\n",
        "for i in range(1,26):\n",
        "    with open('drive/My Drive/Dados/Processos/processos-trt'+str(i)+'.pkl','rb') as f:\n",
        "        df = pd.read_pickle(f)\n",
        "    print(i, df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 (48029, 29)\n",
            "2 (45135, 29)\n",
            "3 (51922, 29)\n",
            "4 (46315, 29)\n",
            "5 (41014, 29)\n",
            "6 (38020, 29)\n",
            "7 (39596, 29)\n",
            "8 (42440, 29)\n",
            "9 (40707, 29)\n",
            "10 (42336, 29)\n",
            "11 (34424, 29)\n",
            "12 (42487, 29)\n",
            "13 (30338, 29)\n",
            "14 (27994, 29)\n",
            "15 (53583, 29)\n",
            "16 (36741, 29)\n",
            "17 (41663, 29)\n",
            "18 (38883, 29)\n",
            "19 (45137, 29)\n",
            "20 (33457, 29)\n",
            "21 (34333, 29)\n",
            "22 (34553, 29)\n",
            "23 (2, 29)\n",
            "24 (33410, 29)\n",
            "25 (59851, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhSudNXuZk_"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ci7ChWFzs06"
      },
      "source": [
        "# Essa função recebe uma lista de paths contendo o caminho para os .pkl que contém os dados dos TRTs e do TST. \n",
        "    # É feita a clusterização e o preenchimento de uma nova coluna, sendo os dados salvos posteriormente\n",
        "\n",
        "def modelo_cluster_processo(lista_paths, predict=True, threshold=0.35,\n",
        "                            branching_factor=50,\n",
        "                            n_clusters=4):\n",
        "    \n",
        "    birch = Birch(n_clusters=n_clusters, threshold=threshold,\n",
        "                  branching_factor=branching_factor)\n",
        "\n",
        "    # Abre os df processados e executa um partial_fit\n",
        "    for path in lista_paths:\n",
        "        with open(path,'rb') as f:\n",
        "            df = pd.read_pickle(f)\n",
        "\n",
        "        df2 = df.copy()\n",
        "        df2 = df[['grau', 'totalAssuntos', 'tempo_proc_dias', 'total_movs_loc', 'total_movs_nac',\n",
        "           'total_movs', '1_mov_nac_freq', '2_mov_nac_freq', 'tempoMedioEntreMovimentos']]\n",
        "\n",
        "        # Normaliza o input\n",
        "        X_norm = normalize(df2.to_numpy(), norm='l2')\n",
        "        # Treino parcial\n",
        "        birch.partial_fit(X_norm)\n",
        "    \n",
        "    #### Se TRUE, faz predição e coloca em uma nova coluna do DF\n",
        "    if predict:\n",
        "        for path in lista_paths:\n",
        "            with open(path, 'rb') as f:\n",
        "                df = pd.read_pickle(f)\n",
        "            \n",
        "            df2 = df.copy()\n",
        "            df2 = df[['grau', 'totalAssuntos', 'tempo_proc_dias', 'total_movs_loc', 'total_movs_nac',\n",
        "           'total_movs', '1_mov_nac_freq', '2_mov_nac_freq', 'tempoMedioEntreMovimentos']]\n",
        "\n",
        "            X = normalize(df2.to_numpy(), norm='l2')\n",
        "            \n",
        "            df['cluster_processo'] = birch.predict(X)\n",
        "            # Código para salvar o arquivo novamente\n",
        "            with open(path, 'wb') as f:\n",
        "                df.to_pickle(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw61y7_my_VC"
      },
      "source": [
        "# Essa função recebe um dos dataframes e calcula estatísticas sobre os clusters.\n",
        "    # o resultado é um .csv para cada dataframe (TRT ou TST), contendo dados sobre os processos em determinado cluster.\n",
        "    # Vale ressaltar que, dado que os clusters foram feitos com base em todos os dados de todos os TRTs e TST, não é necessário criar\n",
        "        # um arquivo para cada TRT/TST. Pelo princípio de amostragem, um dos TRTs, que contenha processos em todos os clusters, expressa a realidade de todos.\n",
        "\n",
        "def cluster_processos_csv(df, trtnum):\n",
        "    # Cria um índice (clusters de processos)\n",
        "    index = df['cluster_processo'].unique()\n",
        "    # Colunas do df de clusters\n",
        "    lista_colunas = ['media_dias', 'media_movs', 'media_movs_loc', 'media_movs_nac', 'media_assuntos', \n",
        "                     'media_dias_entre_movs', 'mov_mais_comum1', 'mov_mais_comum2', 'qtd_processos', 'pontuacao']\n",
        "    # Criando o df e ordenando o índice \n",
        "    cluster_df = pd.DataFrame(index=index, columns=lista_colunas)\n",
        "    cluster_df.sort_index(inplace=True)\n",
        "\n",
        "    for cluster in df['cluster_processo'].unique():\n",
        "\n",
        "        # Preenchendo média de movimentos totais, nacionais e locais\n",
        "        media_nac = df.loc[df['cluster_processo']==cluster, 'total_movs_nac'].mean()\n",
        "        media_loc = df.loc[df['cluster_processo']==cluster, 'total_movs_loc'].mean()\n",
        "        media_tot = df.loc[df['cluster_processo']==cluster, 'total_movs'].mean()\n",
        "        cluster_df.loc[cluster, 'media_movs_loc'] =  media_loc\n",
        "        cluster_df.loc[cluster, 'media_movs_nac'] =  media_nac\n",
        "        cluster_df.loc[cluster, 'media_movs'] =  media_tot\n",
        "\n",
        "        # Preenchendo média do total de assuntos\n",
        "        media_assunto = df.loc[df['cluster_processo']==cluster, 'totalAssuntos'].mean()\n",
        "        cluster_df.loc[cluster, 'media_assuntos'] =  media_assunto\n",
        "\n",
        "        # Preenchendo média dos dias entre movimentos\n",
        "        media_dias = df.loc[df['cluster_processo']==cluster, 'tempo_proc_dias'].mean()\n",
        "        cluster_df.loc[cluster, 'media_dias'] =  media_dias\n",
        "\n",
        "        # Preenchendo média de dias entre movimentos\n",
        "        media_entre_movs = df.loc[df['cluster_processo']==cluster, 'tempoMedioEntreMovimentos'].mean()\n",
        "        cluster_df.loc[cluster, 'media_dias_entre_movs'] =  media_entre_movs\n",
        "\n",
        "        # Preenchendo movimentos mais comuns naquele cluster\n",
        "        mov_1 = df.loc[df['cluster_processo']==cluster, '1_mov_nac_freq'].mode()\n",
        "        mov_2 = df.loc[df['cluster_processo']==cluster, '2_mov_nac_freq'].mode()\n",
        "        cluster_df.loc[cluster, 'mov_mais_comum1'] =  mov_1[0]\n",
        "        cluster_df.loc[cluster, 'mov_mais_comum2'] =  mov_2[0]\n",
        "\n",
        "        # Preenchendo quantidade de processos do cluster\n",
        "        cluster_df.loc[cluster, 'qtd_processos'] = len(df[df['cluster_processo']==cluster])\n",
        "\n",
        "    # Pontuação\n",
        "    lista_dias = list(cluster_df['media_dias'])\n",
        "    lista_dias_ordenada = sorted(lista_dias)\n",
        "    pont_dias = []\n",
        "\n",
        "    lista_diasMovs = list(cluster_df['media_dias_entre_movs'])\n",
        "    lista_diasMovs_ordenada = sorted(lista_diasMovs)\n",
        "    pont_dias_movs = []\n",
        "\n",
        "    lista_movs_nac = list(cluster_df['media_movs_nac'])\n",
        "    lista_movs_nac_ordenada = sorted(lista_movs_nac, reverse=True)\n",
        "    pont_movs_nac = []\n",
        "\n",
        "    lista_qtd_procs = list(cluster_df['qtd_processos'])\n",
        "    lista_qtd_procs_ord = sorted(lista_qtd_procs, reverse=True)\n",
        "    pont_qtd_procs = []\n",
        "\n",
        "    for i in range(len(lista_dias_ordenada)):\n",
        "        pont_dias.append(len(lista_dias_ordenada)*4 - lista_dias.index(lista_dias_ordenada[i])*4)\n",
        "        pont_dias_movs.append(len(lista_diasMovs_ordenada) - lista_diasMovs.index(lista_diasMovs_ordenada[i]))\n",
        "        pont_movs_nac.append(len(lista_movs_nac_ordenada) - lista_movs_nac.index(lista_movs_nac_ordenada[i]))\n",
        "        pont_qtd_procs.append(len(lista_qtd_procs_ord) - lista_qtd_procs.index(lista_qtd_procs_ord[i]))\n",
        "        \n",
        "    l1 = list(map(operator.add, pont_dias, pont_dias_movs))\n",
        "    l2 = list(map(operator.add, pont_movs_nac, pont_qtd_procs))\n",
        "    cluster_df['pontuacao'] = list(map(operator.add, l1, l2))\n",
        "    cluster_df['id_cluster'] = cluster_df.index\n",
        "    cluster_df.to_csv('drive/My Drive/Dados/Estatísticas Clusters/estatisticas-cluster-processos'+trtnum+'.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqdKs__d3Qmt"
      },
      "source": [
        "# Realiza o clustering de unidades judiciais, para o arquivo \"orgaos-julgadores-clusterizados.csv\"\n",
        "    # Depende do arquivo orgaos-julgadores-trabalhista.csv, que é feito pelas funções no notebook Manipulação de CSV\n",
        "   \n",
        "def modelo_cluster_unidade(path = '/content/drive/My Drive/Dados/orgaos-julgadores-trabalhista.csv',\n",
        "                            threshold=0.35, branching_factor=50, n_clusters=4,\n",
        "                            nome_arquivo='/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv'):\n",
        "    \n",
        "    birch = Birch(n_clusters=n_clusters, threshold=threshold,\n",
        "                  branching_factor=branching_factor)\n",
        "    \n",
        "    df = pd.read_csv(path)\n",
        "    df = df[df['qtd_processos'] != 0]\n",
        "    df2 = df[['CAPITAL', 'qtd_processos', 'tempo_entre_movs', 'cluster_processual_mais_comum']]\n",
        "\n",
        "    X_norm = normalize(df2.to_numpy(), norm='l2')\n",
        "    birch.fit(X_norm)\n",
        "\n",
        "    # Predict\n",
        "    df['cluster_unidade'] = birch.predict(X_norm)\n",
        "    df.to_csv(nome_arquivo,index=False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvv3tfc6iKwv"
      },
      "source": [
        "# Recebe um dataframe de processos de um TRT e gera dados relativos aos processos daquele TRT.\n",
        "    # essa função precisa ser invocada para todos os arquivos de TRTs/TST\n",
        "def dados_unidades_judiciais(df):\n",
        "    unidades_julgadoras = pd.read_csv('/content/drive/My Drive/Dados/orgaos-julgadores-trabalhista.csv')\n",
        "\n",
        "    # Preenche os dados dos órgãos julgadores com ids nos processos\n",
        "    for id_orgao in df['codigoOrgaoJulgador'].unique():\n",
        "        # Quantidade de processos\n",
        "        qtd_procs = df.loc[df['codigoOrgaoJulgador']==id_orgao].shape[0]\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==id_orgao, 'qtd_processos'] = qtd_procs\n",
        "\n",
        "        # Tempo médio do processo\n",
        "        tempo_medio = df.loc[df['codigoOrgaoJulgador']==id_orgao, 'tempo_proc_dias'].mean()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==id_orgao, 'tempo_medio_proc'] = tempo_medio\n",
        "\n",
        "        # Tempo entre movimentações\n",
        "        tempo_entre_movs = df.loc[df['codigoOrgaoJulgador']==id_orgao, 'tempoMedioEntreMovimentos'].mean()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==id_orgao, 'tempo_entre_movs'] = tempo_entre_movs\n",
        "\n",
        "        # Cluster processual mais comum\n",
        "        cluster_comum = df.loc[df['codigoOrgaoJulgador'] == id_orgao, 'cluster_processo'].mode()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==id_orgao, 'cluster_processual_mais_comum'] = cluster_comum[0]\n",
        "\n",
        "\n",
        "    # Preenche os dados dos órgãos que são pais dos órgãos julgadores (TRTs e TST)\n",
        "    lista_pais = list(unidades_julgadoras.loc[unidades_julgadoras['qtd_processos'] != 0, 'SEQ_ORGAO_PAI'].unique())\n",
        "\n",
        "    for pai in lista_pais:\n",
        "        # Quantidade de processos\n",
        "        qtd_procs = unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO_PAI'] == int(pai), 'qtd_processos'].sum()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==int(pai), 'qtd_processos'] = qtd_procs\n",
        "\n",
        "        # Tempo médio de vida dos processos\n",
        "        tempo_medio = unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO_PAI'] == int(pai), 'tempo_medio_proc'].mean()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==int(pai), 'tempo_medio_proc'] = tempo_medio\n",
        "\n",
        "        # Tempo médio entre movimentações \n",
        "        tempo_entre_movs = unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO_PAI'] == int(pai), 'tempo_entre_movs'].mean()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==int(pai), 'tempo_entre_movs'] = tempo_entre_movs\n",
        "\n",
        "        # Cluster processual mais comum\n",
        "        cluster_comum = unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO_PAI'] == int(pai), 'cluster_processual_mais_comum'].mode()\n",
        "        unidades_julgadoras.loc[unidades_julgadoras['SEQ_ORGAO']==int(pai), 'cluster_processual_mais_comum'] = cluster_comum[0]\n",
        "\n",
        "    with open('/content/drive/My Drive/Dados/orgaos-julgadores-trabalhista.csv', 'w') as file:\n",
        "        unidades_julgadoras.to_csv(file, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkhTEElEJqFY"
      },
      "source": [
        "# Cria um arquivo .csv contendo informações sobre os clusters de unidades judiciais.\n",
        "    # essa função espera receber um dataframe de orgaos julgadores clusterizados\n",
        "\n",
        "# DF recebido é o dataframe de orgaos julgadores clusterizados\n",
        "\n",
        "def cluster_unidades_csv(df):\n",
        "    # Cria um índice (clusters de processos)\n",
        "    index = df['cluster_unidade'].unique()\n",
        "    # Colunas do df de clusters\n",
        "    lista_colunas = ['media_dias', 'media_dias_entre_movs', 'qtd_media_procs', 'pontuacao_cluster_proc' ,'pontuacao']\n",
        "    # Criando o df e ordenando o índice \n",
        "    cluster_df = pd.DataFrame(index=index, columns=lista_colunas)\n",
        "    cluster_df.sort_index(inplace=True)\n",
        "\n",
        "    cluster_procs = pd.read_csv('/content/drive/My Drive/Dados/estatisticas-cluster-processos1.csv')\n",
        "\n",
        "    for cluster in df['cluster_unidade'].unique():\n",
        "\n",
        "        # Preenchendo média dos dias\n",
        "        media_dias = df.loc[df['cluster_unidade']==cluster, 'tempo_medio_proc'].mean()\n",
        "        cluster_df.loc[cluster, 'media_dias'] =  media_dias\n",
        "\n",
        "        # Preenchendo média de dias entre movimentos\n",
        "        media_entre_movs = df.loc[df['cluster_unidade']==cluster, 'tempo_entre_movs'].mean()\n",
        "        cluster_df.loc[cluster, 'media_dias_entre_movs'] =  media_entre_movs\n",
        "\n",
        "        # Preenche coluna pontuacao_cluster_proc com a pontuação do cluster de processos mais comum naquela unidade judicial\n",
        "        pontuacao_proc = cluster_procs.loc[cluster, 'pontuacao']\n",
        "        cluster_df.loc[cluster, 'pontuacao_cluster_proc'] =  pontuacao_proc\n",
        "\n",
        "        # Média de processos\n",
        "        media_procs = df.loc[df['cluster_unidade']==cluster, 'qtd_processos'].mean()\n",
        "        cluster_df.loc[cluster, 'qtd_media_procs'] = media_procs\n",
        "    \n",
        "\n",
        "    # Pontuação\n",
        "    lista_dias = list(cluster_df['media_dias'])\n",
        "    lista_dias_ordenada = sorted(lista_dias)\n",
        "    pont_dias = []\n",
        "\n",
        "    lista_diasMovs = list(cluster_df['media_dias_entre_movs'])\n",
        "    lista_diasMovs_ordenada = sorted(lista_diasMovs)\n",
        "    pont_dias_movs = []\n",
        "\n",
        "    lista_qtd_procs = list(cluster_df['qtd_media_procs'])\n",
        "    lista_qtd_procs_ord = sorted(lista_qtd_procs, reverse=True)\n",
        "    pont_qtd_procs = []\n",
        "    \n",
        "\n",
        "    for i in range(len(lista_dias_ordenada)):\n",
        "        pont_dias.append(len(lista_dias_ordenada)*4 - lista_dias.index(lista_dias_ordenada[i])*4)\n",
        "        pont_dias_movs.append(len(lista_diasMovs_ordenada) - lista_diasMovs.index(lista_diasMovs_ordenada[i]))\n",
        "        pont_qtd_procs.append(len(lista_qtd_procs_ord) - lista_qtd_procs.index(lista_qtd_procs_ord[i]))\n",
        "\n",
        "    l1 = list(map(operator.add, pont_dias, pont_dias_movs))\n",
        "    l2 = list(cluster_df['pontuacao_cluster_proc'])\n",
        "\n",
        "    op1 = map(operator.add, l1, l2)\n",
        "\n",
        "    cluster_df['pontuacao'] = list(map(operator.add, op1, pont_qtd_procs))\n",
        "    cluster_df['id_cluster'] = cluster_df.index\n",
        "    cluster_df.to_csv('/content/drive/My Drive/Dados/estatisticas-cluster-unidades.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oODhyE0kBQn5"
      },
      "source": [
        "Chamando as funções declaradas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "378cwOiuBdCA",
        "outputId": "32335f5b-9eca-402f-9899-9de201039473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Cria a lista de paths com todos os arquivos .pkl dos TRTs\n",
        "lista_paths = []\n",
        "for file in os.listdir(\"drive/My Drive/Dados/Processos\"):\n",
        "    if file.endswith(\".pkl\"):\n",
        "        lista_paths.append(os.path.join(\"drive/My Drive/Dados/Processos\", file))\n",
        "lista_paths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/Dados/Processos/processos-trt1.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt3.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt6.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt4.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt7.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt8.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt9.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt10.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt11.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt12.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt15.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt16.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt17.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt19.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt20.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt21.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt23.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt22.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt5.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt13.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt18.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt14.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt24.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt2.pkl',\n",
              " 'drive/My Drive/Dados/Processos/processos-trt25.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DWbJnqnB_Rc"
      },
      "source": [
        "# Chamando a função que cria o cluster de processos\n",
        "modelo_cluster_processo(lista_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgiTLHiMQpIl"
      },
      "source": [
        "# Chamando a função que cria dados sobre os clusters de processos\n",
        "for i in range(1,26):\n",
        "    with open('drive/My Drive/Dados/Processos/processos-trt'+str(i)+'.pkl','rb') as f:\n",
        "        df = pd.read_pickle(f)\n",
        "    cluster_processos_csv(df, str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f5L5RLLFzXD",
        "outputId": "b2912653-0e66-4ddd-8115-220e03dc3c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Verificando o tamanho dos arquivos, apenas para fins de checagem\n",
        "for path in lista_paths:\n",
        "    with open(path,'rb') as f:\n",
        "        df = pd.read_pickle(f)\n",
        "    print(path, df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Dados/Processos/processos-trt1.pkl (48029, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt3.pkl (51922, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt4.pkl (46315, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt6.pkl (38020, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt7.pkl (39596, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt8.pkl (42440, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt9.pkl (40707, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt10.pkl (42336, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt11.pkl (34424, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt12.pkl (42487, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt15.pkl (53583, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt16.pkl (36741, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt17.pkl (41663, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt19.pkl (45137, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt20.pkl (33457, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt21.pkl (34333, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt23.pkl (2, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt22.pkl (34553, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt5.pkl (41014, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt13.pkl (30338, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt14.pkl (27994, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt18.pkl (38883, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt24.pkl (33410, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt2.pkl (45135, 30)\n",
            "drive/My Drive/Dados/Processos/processos-trt25.pkl (59851, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0rzPf1ZW5qX"
      },
      "source": [
        "# Chamando a função que cria clusters de unidades judiciais\n",
        "modelo_cluster_unidade(threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc0jiS5WCKis",
        "outputId": "fa8efbf2-e2fc-4b6a-cc7f-338551ed8374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Chamando a função que gera dados de unidades judiciais\n",
        "for path in lista_paths:\n",
        "    with open(path,'rb') as f:\n",
        "        df = pd.read_pickle(f)\n",
        "    dados_unidades_judiciais(df)\n",
        "    print(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Dados/Processos/processos-trt1.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt3.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt6.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt4.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt7.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt8.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt9.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt10.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt11.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt12.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt15.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt16.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt17.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt19.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt20.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt21.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt23.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt22.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt5.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt13.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt18.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt14.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt24.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt2.pkl\n",
            "drive/My Drive/Dados/Processos/processos-trt25.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edGN6v1Hrx6O"
      },
      "source": [
        "# Chamando a função que cria dados dos clusters de unidades \n",
        "path = '/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv'\n",
        "df = pd.read_csv(path)\n",
        "cluster_unidades_csv(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJeRS9VH6IG1"
      },
      "source": [
        "## Pontuação de unidades e processos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHN6B8QUAmPU"
      },
      "source": [
        "# Pontuação do processo (menor = melhor)\n",
        "def pontuacao_processos():\n",
        "\n",
        "    # Leitura do .csv de unidades\n",
        "    unidades = pd.read_csv('/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv')\n",
        "    \n",
        "    # Percorrendo todos os arquivos dos TRTs\n",
        "    for numtrt in range(1, 26):\n",
        "        index_trt = 0\n",
        "        with open('drive/My Drive/Dados/Processos/processos-trt'+str(numtrt)+'.pkl','rb') as f:\n",
        "            df = pd.read_pickle(f)\n",
        "        # Inicializando a coluna de pontuaçao com 0\n",
        "        df['pontuacao_processo'] = 0\n",
        "        print(\"TRT \", numtrt)\n",
        "        # Lendo o arquivo de stats do respectivido TRT\n",
        "        stats = '/content/drive/My Drive/Dados/Estatísticas Clusters/estatisticas-cluster-processos{}.csv'.format(str(numtrt))\n",
        "        stats_df = pd.read_csv(stats, index_col='id_cluster')\n",
        "        # Pegando a maior pontuação de cluster\n",
        "        maior_pontuacao_cluster = max(stats_df['pontuacao'])\n",
        "        \n",
        "\n",
        "        # Percorrendo todos os processos do TRT em questão\n",
        "        index_proc = 0\n",
        "        for processo in df['numero']:\n",
        "            # Captando qual é o cluster do processo\n",
        "            id_cluster = df.loc[index_proc, 'cluster_processo']\n",
        "\n",
        "            # Calculando a pontuação do cluster\n",
        "            pontuacao_cluster = pow(maior_pontuacao_cluster, 2)/(stats_df.loc[id_cluster, 'pontuacao'] + 1)\n",
        "            # Calculando desvio entre o tempo entre movimentos e dias do processo com relação às estatísticas dos processos de unidades similares\n",
        "            desvio_tempo_entre_movimentos = abs(df.loc[index_proc, 'tempoMedioEntreMovimentos']) - abs(stats_df.loc[id_cluster, 'media_dias_entre_movs'])\n",
        "            desvio_tempo_total = abs(df.loc[index_proc, 'tempo_proc_dias']) - abs(stats_df.loc[id_cluster, 'media_dias'])\n",
        "            \n",
        "            # Calculando a pontuação do processo\n",
        "            pontuacao = pontuacao_cluster + desvio_tempo_entre_movimentos/pontuacao_cluster + desvio_tempo_total/pontuacao_cluster\n",
        "            # Atribuindo a pontuação\n",
        "            df.loc[index_proc, 'pontuacao_processo'] = pontuacao\n",
        "            \n",
        "            index_proc+=1 \n",
        "        \n",
        "        # Calculando qual o valor de pontuação dos maiores 10% (no caso, os 10% mais mal avaliados) \n",
        "        quantile = df['pontuacao_processo'].quantile(.9)\n",
        "        # Se for a primeira iteração, esse valor é atribuído a um df novo. Caso contrário, é anexado ao df.\n",
        "        if index_trt == 0:\n",
        "            df_alerta = df.loc[df['pontuacao_processo']>quantile]\n",
        "            df_alerta.reset_index(drop=True, inplace=True)\n",
        "        else:\n",
        "            b = df.loc[df['pontuacao_processo']>quantile]\n",
        "            df_alerta = pd.concat([df_alerta, b], ignore_index=True)\n",
        "            df_alerta.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        # Preenchendo a quantidade de processos em alerta para cada unidade \n",
        "        lst_pai = []\n",
        "        # Percorrendo uma lista contendo os valores únicos de órgão julgador para cada TRT\n",
        "        for id_orgao in df_alerta['codigoOrgaoJulgador'].unique():\n",
        "            # Se o valor for 0, continuar\n",
        "            if id==0:\n",
        "                continue\n",
        "            \n",
        "            # Captando a quantidade de processos em alerta em dado órgão de código id_orgao\n",
        "            qtd_processos_orgao = df_alerta[df_alerta['codigoOrgaoJulgador']==id_orgao].shape[0]\n",
        "            # Anexando no df de unidades\n",
        "            unidades.loc[unidades['SEQ_ORGAO']==id_orgao, 'procs_em_alerta'] = qtd_processos_orgao\n",
        "            \n",
        "            # Capturando informação do pai do órgão julgador\n",
        "            try:\n",
        "                pai = int(unidades.loc[unidades['SEQ_ORGAO']==id_orgao, 'SEQ_ORGAO_PAI'])\n",
        "            except:\n",
        "                continue\n",
        "            if pai in lst_pai:\n",
        "                continue\n",
        "            else:\n",
        "                lst_pai.append(pai)\n",
        "        # Percorrendo a lista de pais\n",
        "        for pai in lst_pai:\n",
        "            # Anexando o total de processos de seus filhos\n",
        "            qtd_pai = unidades.loc[unidades['SEQ_ORGAO_PAI'] == int(pai), 'procs_em_alerta'].sum()\n",
        "            unidades.loc[unidades['SEQ_ORGAO']==pai, 'procs_em_alerta'] == qtd_pai\n",
        "\n",
        "        # Salvando o df como .pkl\n",
        "        with open('drive/My Drive/Dados/Processos/processos-trt'+str(numtrt)+'.pkl','wb') as f:\n",
        "            df.to_pickle(f)\n",
        "        \n",
        "        # Incrementando o índice\n",
        "        index_trt+=1\n",
        "    \n",
        "    # Salvando o dataframe que contém todos os processos em alerta\n",
        "    df_alerta.to_csv('/content/drive/My Drive/Dados/processos-em-alerta.csv', index=False)\n",
        "    # Salvando a nova versão do arquivo de unidades judiciais\n",
        "    unidades.to_csv('/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmwWeuSHd7lA",
        "outputId": "1f552a6a-1fe1-4539-8279-9479dc230458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "pontuacao_processos()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRT  1\n",
            "TRT  2\n",
            "TRT  3\n",
            "TRT  4\n",
            "TRT  5\n",
            "TRT  6\n",
            "TRT  7\n",
            "TRT  8\n",
            "TRT  9\n",
            "TRT  10\n",
            "TRT  11\n",
            "TRT  12\n",
            "TRT  13\n",
            "TRT  14\n",
            "TRT  15\n",
            "TRT  16\n",
            "TRT  17\n",
            "TRT  18\n",
            "TRT  19\n",
            "TRT  20\n",
            "TRT  21\n",
            "TRT  22\n",
            "TRT  23\n",
            "TRT  24\n",
            "TRT  25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUNKCY-f83VA"
      },
      "source": [
        "### Gera as pontuações das unidades\n",
        "\n",
        "# Leitura do arquivo que contém informações sobre as unidades\n",
        "unidades = pd.read_csv('/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv')\n",
        "unidades.to_csv('/content/drive/My Drive/Dados/orgaos-julgadores-clusterizados.csv', index=False)\n",
        "# Leitura do arquivo que contém informações sobre o cluster de unidades\n",
        "cluster_unidades = pd.read_csv('/content/drive/My Drive/Dados/estatisticas-cluster-unidades.csv', index_col='id_cluster')\n",
        "\n",
        "# Inicializando o índice, criando uma coluna e captando o maior valor de pontuação de clusters de unidades\n",
        "index = 0\n",
        "unidades['pontuacao_unidade'] = 0\n",
        "maior_pontuacao_cluster = max(cluster_unidades['pontuacao'])\n",
        "\n",
        "# Percorrendo todas as linhas do dataframe de unidades\n",
        "for row in unidades['NOMEDAVARA']:\n",
        "    # Captando a informação sobre o cluster da unidade\n",
        "    id_cluster = unidades.loc[index, 'cluster_unidade']\n",
        "\n",
        "    # Gerando dados\n",
        "    qtd_processos_alerta = unidades.loc[index, 'procs_em_alerta'] # quantidade de processos em alerta. Menor = melhor\n",
        "    pont_cluster = pow(maior_pontuacao_cluster, 2)/(cluster_unidades.loc[id_cluster, 'pontuacao'] + 1) # Pontuação do cluster, manipulada para: menor = melhor\n",
        "    # Desvio entre o tempo entre movimentos da unidade e o tempo entre movimentos do cluster\n",
        "    desvio_tempo_entre_movimentos = abs(unidades.loc[index, 'tempo_entre_movs']) - abs(cluster_unidades.loc[id_cluster, 'media_dias_entre_movs']) \n",
        "    # Desvio entre o tempo médio total da unidade e o tempo total médio do cluster\n",
        "    desvio_tempo_total = abs(unidades.loc[index, 'tempo_medio_proc']) - abs(cluster_unidades.loc[id_cluster, 'media_dias'])\n",
        "\n",
        "    # Relação entre a quantidade de processos e a quantidade de processos em alerta (0 -> não há processos em alerta, 1 -> todos os processos estão em alerta)\n",
        "    try:\n",
        "        procs_total_alerta = unidades.loc[index, 'qtd_processos']/(qtd_processos_alerta)\n",
        "    except:\n",
        "        procs_total_alerta = 0\n",
        "    \n",
        "    # Calculando a pontuação e a atribuindo à respectiva coluna \n",
        "    pontuacao = qtd_processos_alerta + pont_cluster + (desvio_tempo_entre_movimentos + desvio_tempo_total)*procs_total_alerta\n",
        "    unidades.loc[index, 'pontuacao_unidade'] = pontuacao\n",
        "    index+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkMJP6t4B4ge"
      },
      "source": [
        "### Gerando arquivos de ranking\n",
        "# Ranking 1º Grau\n",
        "varas_t = unidades.loc[unidades['GRAU_ORGAO']=='GRA1T']\n",
        "varas_t['ranking'] = varas_t['pontuacao_unidade'].rank(method='min')\n",
        "varas_t.set_index('ranking', drop=False, inplace=True)\n",
        "varas_t.sort_index(inplace=True)\n",
        "varas_t.to_csv('/content/drive/My Drive/Dados/ranking-1grau.csv', index=True)\n",
        "\n",
        "# Ranking 2º Grau\n",
        "gabs_t = unidades.loc[unidades['GRAU_ORGAO']=='GRA2T']\n",
        "gabs_t['ranking'] = gabs_t['pontuacao_unidade'].rank(method='min')\n",
        "gabs_t.set_index('ranking', drop=False, inplace=True)\n",
        "gabs_t.sort_index(inplace=True)\n",
        "gabs_t.to_csv('/content/drive/My Drive/Dados/ranking-2grau.csv', index=True)\n",
        "\n",
        "# Ranking de Tribunais Federais\n",
        "trts = unidades.loc[unidades['GRAU_ORGAO']=='TRIBT']\n",
        "trts['ranking'] = trts['pontuacao_unidade'].rank(method='min')\n",
        "trts.set_index('ranking', drop=False, inplace=True)\n",
        "trts.sort_index(inplace=True)\n",
        "trts.to_csv('/content/drive/My Drive/Dados/ranking-trts.csv', index=True)\n",
        "\n",
        "# Ranking de Gabinetes de Ministros 3º Grau\n",
        "gabim = unidades.loc[unidades['GRAU_ORGAO']=='GABIM']\n",
        "gabim['ranking'] = gabim['pontuacao_unidade'].rank(method='min')\n",
        "gabim.set_index('ranking', drop=False, inplace=True)\n",
        "gabim.sort_index(inplace=True)\n",
        "gabim.to_csv('/content/drive/My Drive/Dados/ranking-3grau.csv', index=True)\n",
        "\n",
        "# Ranking Geral\n",
        "geral = unidades.copy()\n",
        "geral['ranking'] = geral['pontuacao_unidade'].rank(method='min')\n",
        "geral.set_index('ranking', drop=False, inplace=True)\n",
        "geral.sort_index(inplace=True)\n",
        "geral.to_csv('/content/drive/My Drive/Dados/ranking-geral.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}